{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8708df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing toxicity: 100%|██████████| 500/500 [11:08<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.18 with F1 Score: 0.7617\n",
      "                                            tweet_text  toxicity  \\\n",
      "0    RT @goodfoodAU: A restaurant AND a cookbook fo...  0.010179   \n",
      "1    @Qoloob4 @Vandaliser @sajid_fairooz @IsraeliRe...  0.761980   \n",
      "2    @YouSoFancy oh MAN!! Not sure it gets better t...  0.047358   \n",
      "3    The Bully at Work: What You Can Do to Stop the...  0.146031   \n",
      "4       @harmlesstree2 Here 11 https://t.co/xWJzpSodGj  0.009739   \n",
      "..                                                 ...       ...   \n",
      "495  fuck these bitch ass niggers how yall doing I'...  0.974994   \n",
      "496  @ChrisWarcraft [ i am doing this to make the @...  0.047358   \n",
      "497  Yes, I hated that school. A lot of bullies btw...  0.360951   \n",
      "498  Salty af too! Lol @JayJTheKidd: Damn you mad b...  0.924899   \n",
      "499  #Taliban terrorists killing innocent muslims a...  0.471738   \n",
      "\n",
      "     is_cyberbullying  predicted  \n",
      "0                   0          0  \n",
      "1                   0          0  \n",
      "2                   0          0  \n",
      "3                   0          0  \n",
      "4                   0          0  \n",
      "..                ...        ...  \n",
      "495                 1          1  \n",
      "496                 1          0  \n",
      "497                 1          0  \n",
      "498                 1          1  \n",
      "499                 1          0  \n",
      "\n",
      "[500 rows x 4 columns]\n",
      "Results saved to 'cyberbullying_scored.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# === API Setup ===\n",
    "API_KEY = 'REPLACE_WITH_KEY'\n",
    "PERSPECTIVE_URL = f'https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key={API_KEY}'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load full dataset\n",
    "df_full = pd.read_csv('cyberbullying.csv')\n",
    "\n",
    "# Filter each group\n",
    "df_not_cyber = df_full[df_full['cyberbullying_type'] == 'not_cyberbullying'].sample(n=250, random_state=42)\n",
    "df_cyber = df_full[df_full['cyberbullying_type'] != 'not_cyberbullying'].sample(n=250, random_state=42)\n",
    "\n",
    "# Combine them\n",
    "df = pd.concat([df_not_cyber, df_cyber]).reset_index(drop=True)\n",
    "\n",
    "# === Function to get toxicity score with rate limiting ===\n",
    "def get_toxicity_score(text):\n",
    "    data = {\n",
    "        'comment': {'text': text},\n",
    "        'languages': ['en'],\n",
    "        'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.post(PERSPECTIVE_URL, data=json.dumps(data))\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            return result['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit hit. Sleeping for 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"API Error: {e}\")\n",
    "                return None\n",
    "        finally:\n",
    "            time.sleep(1)  # delay to prevent hitting the rate limit\n",
    "\n",
    "# === Add toxicity score with tqdm progress bar ===\n",
    "if 'toxicity' not in df.columns or df['toxicity'].isnull().any():\n",
    "    tqdm.pandas(desc=\"Analyzing toxicity\")\n",
    "    df['toxicity'] = df['tweet_text'].progress_apply(get_toxicity_score)\n",
    "else:\n",
    "    print(\"Using existing toxicity scores from file.\")\n",
    "\n",
    "# === Label binary ground truth ===\n",
    "df['is_cyberbullying'] = df['cyberbullying_type'].apply(lambda x: 0 if x == 'not_cyberbullying' else 1)\n",
    "\n",
    "# === Find best threshold ===\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "thresholds = [i / 100 for i in range(10, 91)]\n",
    "\n",
    "for t in thresholds:\n",
    "    df['predicted'] = df['toxicity'].apply(lambda x: 1 if x is not None and x >= t else 0)\n",
    "    score = f1_score(df['is_cyberbullying'], df['predicted'])\n",
    "    if score > best_f1:\n",
    "        best_f1 = score\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"Best threshold: {best_threshold} with F1 Score: {best_f1:.4f}\")\n",
    "print(df[['tweet_text', 'toxicity', 'is_cyberbullying', 'predicted']])\n",
    "\n",
    "# === Save final output ===\n",
    "df.to_csv('cyberbullying_scored.csv', index=False)\n",
    "print(\"Results saved to 'cyberbullying_jigsaw_scored.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006453c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
